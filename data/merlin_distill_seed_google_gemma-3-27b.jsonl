{"id": "20260120-193614-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 22911.48042678833, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:36:37Z"}}
{"id": "20260120-193614-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, optimizing efficiency and ensuring timely completion. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 33, "latency_ms": 7780.652761459351, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, optimizing efficiency and ensuring timely completion. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:36:45Z"}}
{"id": "20260120-193614-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2600.076675415039, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:36:47Z"}}
{"id": "20260120-193614-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded JSON data, or None if an error occurs.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 48672.25909233093, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded JSON data, or None if an error occurs.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:37:36Z"}}
{"id": "20260120-193614-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5511.287212371826, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:37:42Z"}}
{"id": "20260120-193614-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script. Make sure this path is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`:  This passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`: This is a crucial flag that instructs the script to evaluate *all* models available on the specified server.  The script needs to be designed to understand and respond to this flag.\n\n**Important Considerations:**\n\n* **Script Functionality:** The `scripts/eval_lmstudio_models.py` script *must* have logic to handle the `--all-models` argument. It should:\n    1. Connect to the server at `http://127.0.0.1:1", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 46695.89447975159, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script. Make sure this path is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`:  This passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`: This is a crucial flag that instructs the script to evaluate *all* models available on the specified server.  The script needs to be designed to understand and respond to this flag.\n\n**Important Considerations:**\n\n* **Script Functionality:** The `scripts/eval_lmstudio_models.py` script *must* have logic to handle the `--all-models` argument. It should:\n    1. Connect to the server at `http://127.0.0.1:1", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:38:29Z"}}
{"id": "20260120-193614-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2457.0181369781494, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:38:32Z"}}
{"id": "20260120-193614-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model architecture that has a reasonable chance of meeting your core performance goals. Complex models take longer to train, debug, and deploy. Faster iteration cycles allow you to quickly test hypotheses, identify issues, and make improvements with limited resources.  Don't reach for Transformers if a good Logistic Regression or Gradient Boosting Machine might suffice.\n\n*   **Focus on Data Efficiency & Transfer Learning:** If possible, leverage pre-trained models (transfer learning) or techniques that require less data (e.g., few-shot learning).  Spending time finding/creating high-quality, *targeted* datasets is often more valuable than scaling up a massive dataset for a model that needs tons of examples. Consider active learning to strategically label the most informative samples.\n\n\n\nThese rules are about being pragmatic and getting value quickly in constrained environments. They emphasize avoiding over-engineering and making smart choices about data usage.", "prompt_tokens": 25, "completion_tokens": 230, "latency_ms": 42887.96138763428, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model architecture that has a reasonable chance of meeting your core performance goals. Complex models take longer to train, debug, and deploy. Faster iteration cycles allow you to quickly test hypotheses, identify issues, and make improvements with limited resources.  Don't reach for Transformers if a good Logistic Regression or Gradient Boosting Machine might suffice.\n\n*   **Focus on Data Efficiency & Transfer Learning:** If possible, leverage pre-trained models (transfer learning) or techniques that require less data (e.g., few-shot learning).  Spending time finding/creating high-quality, *targeted* datasets is often more valuable than scaling up a massive dataset for a model that needs tons of examples. Consider active learning to strategically label the most informative samples.\n\n\n\nThese rules are about being pragmatic and getting value quickly in constrained environments. They emphasize avoiding over-engineering and making smart choices about data usage.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:39:15Z"}}
{"id": "20260120-193614-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6799.712657928467, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:39:22Z"}}
{"id": "20260120-193614-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, optimizing efficiency and ensuring timely completion. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 33, "latency_ms": 7086.225748062134, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, optimizing efficiency and ensuring timely completion. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:39:29Z"}}
{"id": "20260120-193614-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2669.262409210205, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:39:32Z"}}
{"id": "20260120-193614-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurs \n    (e.g., file not found, invalid JSON).\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\") # Catch other potential errors\n    return None\n\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  ", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 49729.04276847839, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurs \n    (e.g., file not found, invalid JSON).\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\") # Catch other potential errors\n    return None\n\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  ", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:40:22Z"}}
{"id": "20260120-193614-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5719.783544540405, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:40:28Z"}}
{"id": "20260120-193614-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.\n* `--server-address http://127.0.0.1:1234`: Passes the server address as an argument to the script, telling it where to connect to LM Studio.  This is crucial for accessing the models.\n* `--all-models`: This flag (assuming your script supports it) instructs the script to evaluate *all* available models on the specified server.\n\n**Important Considerations and Potential Modifications:**\n\n1. **Script Support:** The most important thing is that `scripts/eval_lmstudio_models.py` actually has a `--all-models` option or equivalent functionality.  If it doesn't, you'll need to find another way to iterate through the models.  Check your script's documentation (e.g., using `--help`) to see what options are available.\n\n2. **", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 45263.25988769531, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.\n* `--server-address http://127.0.0.1:1234`: Passes the server address as an argument to the script, telling it where to connect to LM Studio.  This is crucial for accessing the models.\n* `--all-models`: This flag (assuming your script supports it) instructs the script to evaluate *all* available models on the specified server.\n\n**Important Considerations and Potential Modifications:**\n\n1. **Script Support:** The most important thing is that `scripts/eval_lmstudio_models.py` actually has a `--all-models` option or equivalent functionality.  If it doesn't, you'll need to find another way to iterate through the models.  Check your script's documentation (e.g., using `--help`) to see what options are available.\n\n2. **", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:41:14Z"}}
{"id": "20260120-193614-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2301.654815673828, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:41:16Z"}}
{"id": "20260120-193614-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy. Faster iteration allows you to get feedback quickly and pivot if needed \u2013 crucial when resources are limited.  Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained weights) or models inherently requiring less data (e.g., certain tree-based methods, some Bayesian approaches) are preferable. Avoid models that demand massive amounts of labeled data if you don't have it.\n\n\n\nThese rules emphasize getting something *working* and learning from it quickly, rather than aiming for perfection with a resource-intensive approach that might not even be feasible.", "prompt_tokens": 25, "completion_tokens": 215, "latency_ms": 39154.250144958496, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy. Faster iteration allows you to get feedback quickly and pivot if needed \u2013 crucial when resources are limited.  Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained weights) or models inherently requiring less data (e.g., certain tree-based methods, some Bayesian approaches) are preferable. Avoid models that demand massive amounts of labeled data if you don't have it.\n\n\n\nThese rules emphasize getting something *working* and learning from it quickly, rather than aiming for perfection with a resource-intensive approach that might not even be feasible.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:41:56Z"}}
{"id": "20260120-193614-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6875.621557235718, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:42:03Z"}}
{"id": "20260120-193614-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, optimizing efficiency and ensuring timely completion. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 33, "latency_ms": 7017.661571502686, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, optimizing efficiency and ensuring timely completion. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:42:10Z"}}
{"id": "20260120-193614-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2683.73703956604, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:42:13Z"}}
{"id": "20260120-193614-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:\n\n* **Error Handling:**", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 49379.29844856262, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:\n\n* **Error Handling:**", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:43:03Z"}}
{"id": "20260120-193614-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5571.8512535095215, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:43:09Z"}}
{"id": "20260120-193614-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this path is correct relative to where you are running the command from.\n* `--server http://127.0.0.1:1234`: This passes the server address (where LM Studio is running) as an argument to the script.  This is crucial for the script to connect and interact with the models.\n* `--all-models`: This tells the script to evaluate *all* available models on the specified server.\n\n**Important Considerations:**\n\n* **Script Requirements:** Ensure that `scripts/eval_lmstudio_models.py` is designed to accept these arguments (`--server` and `--all-models`).  The script's documentation or source code will tell you what arguments it expects.\n* **LM Studio Server:** Verify that LM Studio is running and accessible at `http://12", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 46309.63969230652, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this path is correct relative to where you are running the command from.\n* `--server http://127.0.0.1:1234`: This passes the server address (where LM Studio is running) as an argument to the script.  This is crucial for the script to connect and interact with the models.\n* `--all-models`: This tells the script to evaluate *all* available models on the specified server.\n\n**Important Considerations:**\n\n* **Script Requirements:** Ensure that `scripts/eval_lmstudio_models.py` is designed to accept these arguments (`--server` and `--all-models`).  The script's documentation or source code will tell you what arguments it expects.\n* **LM Studio Server:** Verify that LM Studio is running and accessible at `http://12", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:43:55Z"}}
{"id": "20260120-193614-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2313.403844833374, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:43:58Z"}}
{"id": "20260120-193614-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy. Faster iteration allows you to quickly test hypotheses, get feedback, and refine based on real-world performance \u2013 crucial when resources are limited.  Think Logistic Regression or basic Decision Trees before jumping to deep learning.\n\n*   **Focus on High-Leverage Features & Data:** Don't try to boil the ocean. Identify the *most important* features for your problem (through domain expertise, quick exploratory data analysis, or feature importance techniques) and prioritize using those with a smaller, well-understood dataset. A highly accurate model trained on relevant, clean data will outperform a complex model trained on lots of noisy, irrelevant data.\n\n\n\nThese rules emphasize getting *something useful* working quickly rather than striving for perfection that may be unattainable given constraints.", "prompt_tokens": 25, "completion_tokens": 225, "latency_ms": 41816.83301925659, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy. Faster iteration allows you to quickly test hypotheses, get feedback, and refine based on real-world performance \u2013 crucial when resources are limited.  Think Logistic Regression or basic Decision Trees before jumping to deep learning.\n\n*   **Focus on High-Leverage Features & Data:** Don't try to boil the ocean. Identify the *most important* features for your problem (through domain expertise, quick exploratory data analysis, or feature importance techniques) and prioritize using those with a smaller, well-understood dataset. A highly accurate model trained on relevant, clean data will outperform a complex model trained on lots of noisy, irrelevant data.\n\n\n\nThese rules emphasize getting *something useful* working quickly rather than striving for perfection that may be unattainable given constraints.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:44:40Z"}}
{"id": "20260120-193614-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6674.638271331787, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:44:47Z"}}
{"id": "20260120-193614-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system automatically assigns incoming tasks or requests to the most appropriate person or team based on predefined rules and criteria, ensuring efficient workflow and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 35, "latency_ms": 7985.238313674927, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system automatically assigns incoming tasks or requests to the most appropriate person or team based on predefined rules and criteria, ensuring efficient workflow and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:44:55Z"}}
{"id": "20260120-193614-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2811.708688735962, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:44:58Z"}}
{"id": "20260120-193614-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded JSON data.  Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:\n\n* **Error Handling:** The", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 47198.341369628906, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded JSON data.  Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:\n\n* **Error Handling:** The", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:45:45Z"}}
{"id": "20260120-193614-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5469.119548797607, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:45:51Z"}}
{"id": "20260120-193614-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server http://127.0.0.1:1234`: Passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`:  This flag (assuming your script supports it) instructs the script to evaluate *all* models available on the specified server.\n\n**Important Considerations and Potential Adjustments:**\n\n1. **Script Support for `--all-models`:** The most crucial part is whether `scripts/eval_lmstudio_models.py` actually has a flag like `--all-models`.  If it doesn't, you'll need to find another way to tell the script which models to evaluate.  Here are some alternatives:\n\n   * **Model", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 44978.506088256836, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server http://127.0.0.1:1234`: Passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`:  This flag (assuming your script supports it) instructs the script to evaluate *all* models available on the specified server.\n\n**Important Considerations and Potential Adjustments:**\n\n1. **Script Support for `--all-models`:** The most crucial part is whether `scripts/eval_lmstudio_models.py` actually has a flag like `--all-models`.  If it doesn't, you'll need to find another way to tell the script which models to evaluate.  Here are some alternatives:\n\n   * **Model", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:46:36Z"}}
{"id": "20260120-193614-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2312.220573425293, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:46:39Z"}}
{"id": "20260120-193614-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model architecture that has a reasonable chance of meeting your core performance goals. Complex models take longer to train, debug, and iterate on.  Faster iteration cycles mean you can learn more quickly from limited data/compute and adapt to changing requirements. Think Logistic Regression or basic Decision Trees before jumping to deep neural networks unless there's strong justification.\n\n*   **Focus on High-Leverage Features & Data:** Instead of trying a wide range of models with *all* your available features, concentrate on identifying the 20% of features that likely drive 80% of the signal.  Similarly, if you have limited data, prioritize using the most representative/highest quality subset first. A well-chosen simple model trained on good data will often outperform a complex model trained on noisy or irrelevant information.\n\n\n\nThese rules are about being pragmatic and getting *something useful* deployed quickly rather than chasing theoretical perfection when constrained.", "prompt_tokens": 25, "completion_tokens": 237, "latency_ms": 42738.94810676575, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model architecture that has a reasonable chance of meeting your core performance goals. Complex models take longer to train, debug, and iterate on.  Faster iteration cycles mean you can learn more quickly from limited data/compute and adapt to changing requirements. Think Logistic Regression or basic Decision Trees before jumping to deep neural networks unless there's strong justification.\n\n*   **Focus on High-Leverage Features & Data:** Instead of trying a wide range of models with *all* your available features, concentrate on identifying the 20% of features that likely drive 80% of the signal.  Similarly, if you have limited data, prioritize using the most representative/highest quality subset first. A well-chosen simple model trained on good data will often outperform a complex model trained on noisy or irrelevant information.\n\n\n\nThese rules are about being pragmatic and getting *something useful* deployed quickly rather than chasing theoretical perfection when constrained.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:47:22Z"}}
{"id": "20260120-193614-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 8091.656684875488, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:47:30Z"}}
{"id": "20260120-193614-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 8149.261713027954, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:47:39Z"}}
{"id": "20260120-193614-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 3232.18035697937, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:47:42Z"}}
{"id": "20260120-193614-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded JSON data.  Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:\n\n* **Error Handling:** The", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 48709.396839141846, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded JSON data.  Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:\n\n* **Error Handling:** The", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:48:31Z"}}
{"id": "20260120-193614-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5510.514497756958, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:48:37Z"}}
{"id": "20260120-193614-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`:  This is the crucial part! It instructs the script to evaluate *all* models available on the specified server.  The script must be designed to recognize and handle this flag.\n\n**Important Considerations:**\n\n* **Script Functionality:** This command assumes that `scripts/eval_lmstudio_models.py` has been written to accept `--server-address` and `--all-models` as arguments, and that it correctly iterates through all models on the server when `--all-models` is provided.  ", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 45222.72253036499, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`:  This is the crucial part! It instructs the script to evaluate *all* models available on the specified server.  The script must be designed to recognize and handle this flag.\n\n**Important Considerations:**\n\n* **Script Functionality:** This command assumes that `scripts/eval_lmstudio_models.py` has been written to accept `--server-address` and `--all-models` as arguments, and that it correctly iterates through all models on the server when `--all-models` is provided.  ", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:49:22Z"}}
{"id": "20260120-193614-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2387.882947921753, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:49:25Z"}}
{"id": "20260120-193614-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed to Iterate:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles allow you to get feedback quickly and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained models) or models with strong regularization can be lifesavers. Avoid models that demand massive amounts of labeled data if you don't have it, as acquiring more data is often the biggest resource sink.\n\n\n\nThese rules emphasize getting something working quickly and efficiently rather than aiming for a perfect solution upfront when constrained by resources.", "prompt_tokens": 25, "completion_tokens": 203, "latency_ms": 36568.222522735596, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed to Iterate:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles allow you to get feedback quickly and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained models) or models with strong regularization can be lifesavers. Avoid models that demand massive amounts of labeled data if you don't have it, as acquiring more data is often the biggest resource sink.\n\n\n\nThese rules emphasize getting something working quickly and efficiently rather than aiming for a perfect solution upfront when constrained by resources.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260120-193614", "created_at": "2026-01-20T19:50:02Z"}}
{"id": "20260121-001347-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 23259.87696647644, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-001347", "created_at": "2026-01-21T00:14:10Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 21970.55220603943, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:39:27Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 7904.884576797485, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:39:35Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2444.3957805633545, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:39:38Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurred \n    (e.g., file not found, invalid JSON).\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\") # Catch other potential errors\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  ", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 46753.2913684845, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurred \n    (e.g., file not found, invalid JSON).\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\") # Catch other potential errors\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  ", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:40:25Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 6322.561025619507, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:40:31Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`:  This is the crucial part! It tells the `eval_lmstudio_models.py` script to evaluate *all* models available on the specified server.\n\n**Important Considerations:**\n\n* **Script Requirements:** This command assumes that your `scripts/eval_lmstudio_models.py` script is designed to accept `--server-address` and `--all-models` as arguments, and that it handles connecting to LM Studio at the given address and iterating through all available models.  If the argument names", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 45620.5894947052, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`:  This is the crucial part! It tells the `eval_lmstudio_models.py` script to evaluate *all* models available on the specified server.\n\n**Important Considerations:**\n\n* **Script Requirements:** This command assumes that your `scripts/eval_lmstudio_models.py` script is designed to accept `--server-address` and `--all-models` as arguments, and that it handles connecting to LM Studio at the given address and iterating through all available models.  If the argument names", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:41:17Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2259.7270011901855, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:41:20Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback sooner and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained weights) or models inherently less prone to overfitting (e.g., regularized linear models, certain tree-based methods) are valuable. Avoid models that demand massive amounts of labeled data if you don't have it.\n\n\n\nThese rules emphasize getting something *working* quickly and reliably over chasing the highest theoretical accuracy with a resource-intensive approach.  They align well with an MVP (Minimum Viable Product) mindset.", "prompt_tokens": 25, "completion_tokens": 225, "latency_ms": 40637.37654685974, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback sooner and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained weights) or models inherently less prone to overfitting (e.g., regularized linear models, certain tree-based methods) are valuable. Avoid models that demand massive amounts of labeled data if you don't have it.\n\n\n\nThese rules emphasize getting something *working* quickly and reliably over chasing the highest theoretical accuracy with a resource-intensive approach.  They align well with an MVP (Minimum Viable Product) mindset.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:42:00Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 7497.618198394775, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:42:08Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 6903.955459594727, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:42:15Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2375.7882118225098, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:42:18Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads the model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurs \n    (e.g., file not found, invalid JSON).\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\") # Catch other potential errors\n    return None\n\n\nif __name__ == '__main__':\n  # Example usage:\n  config_data = load_model_routing_config()\n\n  if config_data", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 46962.57448196411, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads the model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurs \n    (e.g., file not found, invalid JSON).\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\") # Catch other potential errors\n    return None\n\n\nif __name__ == '__main__':\n  # Example usage:\n  config_data = load_model_routing_config()\n\n  if config_data", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:43:05Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5294.39902305603, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:43:11Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script. Make sure this is correct relative to where you are running the command from.\n* `--server-address http://127.0.0.1:1234`:  This argument tells the script to connect to the LM Studio server at the specified address.\n* `--all-models`: This argument instructs the script to evaluate *all* models available on the connected server.\n\n**Important Considerations:**\n\n* **Script Dependency:** Ensure that `scripts/eval_lmstudio_models.py` exists and is executable, and has all its required dependencies installed (e.g., any libraries it imports).\n* **LM Studio Server:**  Confirm that your LM Studio server is running at `http://127.0.0.1:1234`. If you've configured a different port or address", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 45459.407329559326, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script. Make sure this is correct relative to where you are running the command from.\n* `--server-address http://127.0.0.1:1234`:  This argument tells the script to connect to the LM Studio server at the specified address.\n* `--all-models`: This argument instructs the script to evaluate *all* models available on the connected server.\n\n**Important Considerations:**\n\n* **Script Dependency:** Ensure that `scripts/eval_lmstudio_models.py` exists and is executable, and has all its required dependencies installed (e.g., any libraries it imports).\n* **LM Studio Server:**  Confirm that your LM Studio server is running at `http://127.0.0.1:1234`. If you've configured a different port or address", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:43:56Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2314.791202545166, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:43:59Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration allows you to get feedback quickly and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained models) or models specifically designed for low-data scenarios (e.g., certain Bayesian methods, meta-learning approaches) can be game-changers when you don't have a lot of labeled data to work with.\n\n\n\nThese rules emphasize getting something *working* quickly and efficiently rather than aiming for the theoretically \"best\" model that might be impractical given constraints.", "prompt_tokens": 25, "completion_tokens": 213, "latency_ms": 40107.18274116516, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration allows you to get feedback quickly and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained models) or models specifically designed for low-data scenarios (e.g., certain Bayesian methods, meta-learning approaches) can be game-changers when you don't have a lot of labeled data to work with.\n\n\n\nThese rules emphasize getting something *working* quickly and efficiently rather than aiming for the theoretically \"best\" model that might be impractical given constraints.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:44:39Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6656.155347824097, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:44:46Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 6904.012680053711, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:44:53Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2424.680709838867, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:44:56Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurs \n    (e.g., file not found, invalid JSON).\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n      print(f\"An unexpected error occurred: {e}\")\n      return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 47397.99427986145, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurs \n    (e.g., file not found, invalid JSON).\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n      print(f\"An unexpected error occurred: {e}\")\n      return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:45:44Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5294.0568923950195, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:45:49Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python scripts/eval_lmstudio_models.py`:  This executes the Python script located at `scripts/eval_lmstudio_models.py`.\n* `--server-address http://127.0.0.1:1234`: This passes the server address to the script, telling it where to find the LM Studio server.\n* `--all-models`:  This is likely a flag within the `eval_lmstudio_models.py` script that instructs it to evaluate *all* models available on the specified server.  The exact behavior of this flag depends on how the script is written, but it's the standard way to indicate evaluating all models when an explicit list isn't provided.\n\n**Important Considerations:**\n\n* **Script Dependency:** This command assumes that `scripts/eval_lmstudio_models.py` exists in your current working directory and has the necessary dependencies installed (e.g., Python libraries).\n* **LM Studio Server:**  Make sure", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 46144.69909667969, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python scripts/eval_lmstudio_models.py`:  This executes the Python script located at `scripts/eval_lmstudio_models.py`.\n* `--server-address http://127.0.0.1:1234`: This passes the server address to the script, telling it where to find the LM Studio server.\n* `--all-models`:  This is likely a flag within the `eval_lmstudio_models.py` script that instructs it to evaluate *all* models available on the specified server.  The exact behavior of this flag depends on how the script is written, but it's the standard way to indicate evaluating all models when an explicit list isn't provided.\n\n**Important Considerations:**\n\n* **Script Dependency:** This command assumes that `scripts/eval_lmstudio_models.py` exists in your current working directory and has the necessary dependencies installed (e.g., Python libraries).\n* **LM Studio Server:**  Make sure", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:46:36Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2348.5686779022217, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:46:38Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and adjust direction more efficiently with limited resources. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. This could include techniques like transfer learning (leveraging pre-trained weights), regularization methods, or models inherently less prone to overfitting (e.g., linear models). Avoid data-hungry architectures unless you have a clear plan for augmentation or synthetic data generation.\n\n\n\nThese rules emphasize getting something *working* and learning quickly over aiming for the absolute best possible performance upfront when resources are constrained.", "prompt_tokens": 25, "completion_tokens": 206, "latency_ms": 38200.72388648987, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and adjust direction more efficiently with limited resources. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. This could include techniques like transfer learning (leveraging pre-trained weights), regularization methods, or models inherently less prone to overfitting (e.g., linear models). Avoid data-hungry architectures unless you have a clear plan for augmentation or synthetic data generation.\n\n\n\nThese rules emphasize getting something *working* and learning quickly over aiming for the absolute best possible performance upfront when resources are constrained.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:47:17Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6452.556133270264, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:47:23Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, optimizing efficiency and ensuring timely completion. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 33, "latency_ms": 6655.728101730347, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, optimizing efficiency and ensuring timely completion. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:47:30Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2520.836353302002, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:47:33Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the model routing configuration, or None if an error occurred.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 47504.94694709778, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the model routing configuration, or None if an error occurred.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n    return None\n\n# Example usage:\nconfig = load_model_routing_config()\n\nif config:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config)\nelse:\n  print(\"Failed to load", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:48:21Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5282.3522090911865, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:48:26Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to the evaluation script.\n* `--server-address http://127.0.0.1:1234`:  Passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`: This is likely a flag within `eval_lmstudio_models.py` that instructs the script to automatically discover and evaluate all models served by the specified server.  The exact behavior of this flag depends on how the script is written; it might iterate through an API endpoint listing available models or use some other method to find them.\n\n**Important Considerations:**\n\n* **Script's Implementation:** This command assumes that `scripts/eval_lmstudio_models.py` *actually has* a `--all-models` flag and that it functions as described (discovering all models from the server", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 45285.72869300842, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to the evaluation script.\n* `--server-address http://127.0.0.1:1234`:  Passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`: This is likely a flag within `eval_lmstudio_models.py` that instructs the script to automatically discover and evaluate all models served by the specified server.  The exact behavior of this flag depends on how the script is written; it might iterate through an API endpoint listing available models or use some other method to find them.\n\n**Important Considerations:**\n\n* **Script's Implementation:** This command assumes that `scripts/eval_lmstudio_models.py` *actually has* a `--all-models` flag and that it functions as described (discovering all models from the server", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:49:12Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2361.098051071167, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:49:15Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration allows you to quickly test hypotheses, gather feedback, and refine based on real-world performance \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency & Transfer Learning:** If possible, leverage pre-trained models (transfer learning) or models known for good performance with *small datasets*.  Spending time on data augmentation or choosing architectures designed for low-data regimes is often more valuable than trying to train a large model from scratch when you're resource-constrained.\n\n\n\nThese rules emphasize getting something *working* quickly and efficiently, rather than aiming for the theoretically \"best\" solution that might be unattainable with limited resources.", "prompt_tokens": 25, "completion_tokens": 220, "latency_ms": 40844.295024871826, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration allows you to quickly test hypotheses, gather feedback, and refine based on real-world performance \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency & Transfer Learning:** If possible, leverage pre-trained models (transfer learning) or models known for good performance with *small datasets*.  Spending time on data augmentation or choosing architectures designed for low-data regimes is often more valuable than trying to train a large model from scratch when you're resource-constrained.\n\n\n\nThese rules emphasize getting something *working* quickly and efficiently, rather than aiming for the theoretically \"best\" solution that might be unattainable with limited resources.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:49:56Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6522.685527801514, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:50:02Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 7232.124090194702, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:50:10Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2534.562587738037, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:50:13Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or parsing fails.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 55140.69437980652, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or parsing fails.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:51:08Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5354.597806930542, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:51:14Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: Passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`:  This is the key part! It instructs the script to evaluate *all* models available on the specified server.  The script needs to be designed to recognize and handle this flag.\n\n**Important Considerations:**\n\n* **Script Functionality:** This command assumes that `scripts/eval_lmstudio_models.py` has been written to accept `--server-address` and `--all-models` as arguments, and that it correctly iterates through all models on the server when `--all-models` is provided.  If your script uses different", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 48464.51950073242, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: Passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`:  This is the key part! It instructs the script to evaluate *all* models available on the specified server.  The script needs to be designed to recognize and handle this flag.\n\n**Important Considerations:**\n\n* **Script Functionality:** This command assumes that `scripts/eval_lmstudio_models.py` has been written to accept `--server-address` and `--all-models` as arguments, and that it correctly iterates through all models on the server when `--all-models` is provided.  If your script uses different", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:52:02Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2315.009593963623, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:52:05Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or basic Decision Trees before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select a model known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained models) or models designed for few-shot/zero-shot learning can be game-changers when you don't have tons of labeled data. Avoid models that are notoriously data-hungry unless you have a very clear plan to augment your existing data effectively.\n\n\n\nThese rules emphasize getting *something useful* into production quickly, rather than aiming for perfection with a resource-intensive approach that might never materialize.", "prompt_tokens": 25, "completion_tokens": 221, "latency_ms": 39755.48076629639, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or basic Decision Trees before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select a model known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained models) or models designed for few-shot/zero-shot learning can be game-changers when you don't have tons of labeled data. Avoid models that are notoriously data-hungry unless you have a very clear plan to augment your existing data effectively.\n\n\n\nThese rules emphasize getting *something useful* into production quickly, rather than aiming for perfection with a resource-intensive approach that might never materialize.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:52:45Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6457.810401916504, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:52:52Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 7450.644254684448, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:52:59Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2453.5529613494873, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:53:02Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n\nif __name__ == '__main__':\n  # Example usage:\n  config_data = load_model_routing_config()\n\n  if config_data:\n    print(\"Model Routing Configuration:\")\n    print(config_data)\n  else:\n    print(\"Failed to load model", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 46888.946294784546, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n\nif __name__ == '__main__':\n  # Example usage:\n  config_data = load_model_routing_config()\n\n  if config_data:\n    print(\"Model Routing Configuration:\")\n    print(config_data)\n  else:\n    print(\"Failed to load model", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:53:49Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5642.552375793457, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:53:55Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This passes the server address as an argument to the script, telling it where to find the LM Studio API.\n* `--all-models`:  This is a crucial flag that tells the script to evaluate *all* models available on the specified server. The script needs to be designed to recognize and handle this flag correctly (e.g., by listing all models from the server).\n\n**Important Considerations:**\n\n* **Script Implementation:** This command assumes your `scripts/eval_lmstudio_models.py` script is written to accept `--all-models` as an argument and then automatically discover and iterate through all available models on the LM Studio server", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 45863.53254318237, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This passes the server address as an argument to the script, telling it where to find the LM Studio API.\n* `--all-models`:  This is a crucial flag that tells the script to evaluate *all* models available on the specified server. The script needs to be designed to recognize and handle this flag correctly (e.g., by listing all models from the server).\n\n**Important Considerations:**\n\n* **Script Implementation:** This command assumes your `scripts/eval_lmstudio_models.py` script is written to accept `--all-models` as an argument and then automatically discover and iterate through all available models on the LM Studio server", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:54:41Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2351.531744003296, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:54:44Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model architecture that has a reasonable chance of meeting your core performance requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles (training/evaluating quickly) are crucial when you can't afford extensive experimentation. Think Logistic Regression, simple Decision Trees, or smaller Neural Networks before jumping to Transformers.\n\n*   **Focus on Data Efficiency & Transfer Learning:** If possible, leverage pre-trained models and transfer learning techniques. Fine-tuning a model already trained on a related dataset will almost always be faster and require less data than training from scratch.  Also, consider models known for good performance with limited data (e.g., certain regularization techniques in linear models, or meta-learning approaches if applicable).\n\n\n\nThese rules emphasize getting *something* working quickly and reliably, rather than chasing the absolute best possible result that might be unattainable given constraints.", "prompt_tokens": 25, "completion_tokens": 227, "latency_ms": 41235.75782775879, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model architecture that has a reasonable chance of meeting your core performance requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles (training/evaluating quickly) are crucial when you can't afford extensive experimentation. Think Logistic Regression, simple Decision Trees, or smaller Neural Networks before jumping to Transformers.\n\n*   **Focus on Data Efficiency & Transfer Learning:** If possible, leverage pre-trained models and transfer learning techniques. Fine-tuning a model already trained on a related dataset will almost always be faster and require less data than training from scratch.  Also, consider models known for good performance with limited data (e.g., certain regularization techniques in linear models, or meta-learning approaches if applicable).\n\n\n\nThese rules emphasize getting *something* working quickly and reliably, rather than chasing the absolute best possible result that might be unattainable given constraints.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:55:25Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6743.352890014648, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:55:32Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 6962.292194366455, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:55:40Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2449.8679637908936, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:55:42Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: Config file not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 46566.51997566223, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: Config file not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:56:29Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 6383.311033248901, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:56:36Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.\n* `--server http://127.0.0.1:1234`: Passes the server address as an argument to the script. This tells the script where to connect to LM Studio's API.  Crucially, this assumes that `eval_lmstudio_models.py` is designed to accept a `--server` argument for specifying the API endpoint.\n* `--all-models`:  This flag instructs the script to evaluate *all* models available on the specified server. This also assumes your script has logic to handle this flag and query the LM Studio API for a list of models if no specific model names are provided.\n\n**Important Considerations:**\n\n1. **Script's Argument Handling:** The command relies heavily on how `eval_lmstudio_models.py` is written.  Make sure it actually *has* `--server` and", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 44689.68057632446, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.\n* `--server http://127.0.0.1:1234`: Passes the server address as an argument to the script. This tells the script where to connect to LM Studio's API.  Crucially, this assumes that `eval_lmstudio_models.py` is designed to accept a `--server` argument for specifying the API endpoint.\n* `--all-models`:  This flag instructs the script to evaluate *all* models available on the specified server. This also assumes your script has logic to handle this flag and query the LM Studio API for a list of models if no specific model names are provided.\n\n**Important Considerations:**\n\n1. **Script's Argument Handling:** The command relies heavily on how `eval_lmstudio_models.py` is written.  Make sure it actually *has* `--server` and", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:57:21Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2370.640277862549, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:57:23Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration allows you to quickly test hypotheses, gather feedback, and refine based on real-world performance \u2013 crucial when resources are limited. Think Logistic Regression or simple Decision Trees before jumping to deep learning.\n\n*   **Focus on High-Leverage Features & Data:** Don't try to boil the ocean. Identify the *most important* features for your problem (through domain expertise, quick exploratory analysis, or feature importance techniques) and prioritize models that can effectively utilize them.  If data is limited, focus on quality over quantity; a smaller, well-labeled dataset with key features will often outperform a large, noisy one.\n\n\n\nThese rules emphasize getting *something useful* into production quickly rather than striving for the theoretically \"best\" model that might never materialize given constraints.", "prompt_tokens": 25, "completion_tokens": 233, "latency_ms": 43217.538595199585, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration allows you to quickly test hypotheses, gather feedback, and refine based on real-world performance \u2013 crucial when resources are limited. Think Logistic Regression or simple Decision Trees before jumping to deep learning.\n\n*   **Focus on High-Leverage Features & Data:** Don't try to boil the ocean. Identify the *most important* features for your problem (through domain expertise, quick exploratory analysis, or feature importance techniques) and prioritize models that can effectively utilize them.  If data is limited, focus on quality over quantity; a smaller, well-labeled dataset with key features will often outperform a large, noisy one.\n\n\n\nThese rules emphasize getting *something useful* into production quickly rather than striving for the theoretically \"best\" model that might never materialize given constraints.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:58:07Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6376.270055770874, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:58:13Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 6830.102920532227, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:58:21Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2409.9347591400146, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:58:23Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads the model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON in file {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 46890.199422836304, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads the model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON in file {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:59:10Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5206.830024719238, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T00:59:16Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script. Make sure this path is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`:  This passes the server address as an argument to the script, telling it where to find the LM Studio API.\n* `--all-models`: This is a crucial flag that instructs the script to evaluate *all* models available on the specified server.\n\n**Important Considerations:**\n\n* **Script Requirements:** Ensure `scripts/eval_lmstudio_models.py` is designed to accept these arguments (`--server-address` and `--all-models`).  The script needs to be able to query the LM Studio API at that address to get a list of models, then iterate through them for evaluation.\n* **LM Studio Server:** Verify that", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 44978.665351867676, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script. Make sure this path is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`:  This passes the server address as an argument to the script, telling it where to find the LM Studio API.\n* `--all-models`: This is a crucial flag that instructs the script to evaluate *all* models available on the specified server.\n\n**Important Considerations:**\n\n* **Script Requirements:** Ensure `scripts/eval_lmstudio_models.py` is designed to accept these arguments (`--server-address` and `--all-models`).  The script needs to be able to query the LM Studio API at that address to get a list of models, then iterate through them for evaluation.\n* **LM Studio Server:** Verify that", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:00:01Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2366.2431240081787, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:00:04Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and adjust direction more efficiently with limited resources. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. This could include techniques like transfer learning (leveraging pre-trained weights), regularization methods, or models inherently less prone to overfitting (e.g., Naive Bayes). Avoid data-hungry algorithms if you can't realistically expand your training data.\n\n\n\nThese rules emphasize getting something *working* and learning quickly over pursuing the theoretically \"best\" model that might be impractical given constraints.", "prompt_tokens": 25, "completion_tokens": 206, "latency_ms": 38706.34698867798, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and adjust direction more efficiently with limited resources. Think Logistic Regression or a small Decision Tree before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. This could include techniques like transfer learning (leveraging pre-trained weights), regularization methods, or models inherently less prone to overfitting (e.g., Naive Bayes). Avoid data-hungry algorithms if you can't realistically expand your training data.\n\n\n\nThese rules emphasize getting something *working* and learning quickly over pursuing the theoretically \"best\" model that might be impractical given constraints.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:00:43Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6581.998825073242, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:00:49Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 6890.062093734741, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:00:57Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2589.7090435028076, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:00:59Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 47464.853048324585, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:01:47Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5860.074520111084, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:01:53Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to execute the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script. Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`:  This passes the server address (where LM Studio is running) as an argument to the script. This tells the script which API endpoint to connect to.\n* `--all-models`: This crucial flag instructs the script to evaluate *all* models available on the specified server. The script likely has logic built in to discover and iterate through them when this flag is present.\n\n**Important Considerations:**\n\n* **Script's Implementation:**  This command assumes that `scripts/eval_lmstudio_models.py` actually supports a `--all-models` option or similar functionality for discovering models from the server. Check the script's documentation (e.", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 45365.74387550354, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to execute the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script. Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`:  This passes the server address (where LM Studio is running) as an argument to the script. This tells the script which API endpoint to connect to.\n* `--all-models`: This crucial flag instructs the script to evaluate *all* models available on the specified server. The script likely has logic built in to discover and iterate through them when this flag is present.\n\n**Important Considerations:**\n\n* **Script's Implementation:**  This command assumes that `scripts/eval_lmstudio_models.py` actually supports a `--all-models` option or similar functionality for discovering models from the server. Check the script's documentation (e.", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:02:39Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2287.6415252685547, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:02:41Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, focusing on practicality and getting *something* useful deployed:\n\n*   **Prioritize Simplicity & Speed to Baseline:**  Choose the simplest model architecture that can reasonably achieve a minimally acceptable performance level. Don't reach for complex transformers if a simpler linear model or decision tree gets you 80% of the way there quickly. The goal is rapid iteration and getting *something* into production to learn from real-world data, even if it isn\u2019t perfect.  Focus on fast training/fine-tuning cycles.\n\n*   **Leverage Pre-trained Models & Transfer Learning:**  Instead of training from scratch, heavily favor pre-trained models (even if not perfectly suited) that can be fine-tuned with your limited data. This drastically reduces resource requirements and often leads to better initial performance than starting anew. Look for models already trained on similar datasets or tasks.\n\n\n\nThese rules emphasize getting a working model deployed quickly so you can start collecting feedback, identifying edge cases, and iterating based on real-world usage \u2013 which is crucial when resources are constrained.  Perfection is the enemy of good in these scenarios!", "prompt_tokens": 25, "completion_tokens": 255, "latency_ms": 46665.25387763977, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, focusing on practicality and getting *something* useful deployed:\n\n*   **Prioritize Simplicity & Speed to Baseline:**  Choose the simplest model architecture that can reasonably achieve a minimally acceptable performance level. Don't reach for complex transformers if a simpler linear model or decision tree gets you 80% of the way there quickly. The goal is rapid iteration and getting *something* into production to learn from real-world data, even if it isn\u2019t perfect.  Focus on fast training/fine-tuning cycles.\n\n*   **Leverage Pre-trained Models & Transfer Learning:**  Instead of training from scratch, heavily favor pre-trained models (even if not perfectly suited) that can be fine-tuned with your limited data. This drastically reduces resource requirements and often leads to better initial performance than starting anew. Look for models already trained on similar datasets or tasks.\n\n\n\nThese rules emphasize getting a working model deployed quickly so you can start collecting feedback, identifying edge cases, and iterating based on real-world usage \u2013 which is crucial when resources are constrained.  Perfection is the enemy of good in these scenarios!", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:03:28Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6403.269529342651, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:03:35Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 7167.963266372681, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:03:42Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 3455.1961421966553, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:03:46Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary containing the loaded configuration, or None if an error occurs.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: Config file not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n    return None\n\n\nif __name__ == '__main__':\n  # Example usage:\n  config = load_model_routing_config()\n\n  if config:\n    print(\"Model routing configuration loaded successfully:\")\n    print(config)\n  else:\n    print", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 47551.89847946167, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary containing the loaded configuration, or None if an error occurs.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: Config file not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n    return None\n\n\nif __name__ == '__main__':\n  # Example usage:\n  config = load_model_routing_config()\n\n  if config:\n    print(\"Model routing configuration loaded successfully:\")\n    print(config)\n  else:\n    print", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:04:34Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5574.179887771606, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:04:40Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This passes the server address (where LM Studio is running) as an argument to the script.  This tells the script *where* to connect to get the model list and evaluate them.\n* `--all-models`: This crucial flag instructs the script to automatically discover and evaluate all models available on the specified server.\n\n**Important Considerations:**\n\n* **Script Requirements:** Ensure that `scripts/eval_lmstudio_models.py` is designed to accept these arguments (`--server-address`, `--all-models`).  The script needs to be able to parse command line arguments using a library like `argparse`.\n* **LM Studio Server", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 46523.85926246643, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This passes the server address (where LM Studio is running) as an argument to the script.  This tells the script *where* to connect to get the model list and evaluate them.\n* `--all-models`: This crucial flag instructs the script to automatically discover and evaluate all models available on the specified server.\n\n**Important Considerations:**\n\n* **Script Requirements:** Ensure that `scripts/eval_lmstudio_models.py` is designed to accept these arguments (`--server-address`, `--all-models`).  The script needs to be able to parse command line arguments using a library like `argparse`.\n* **LM Studio Server", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:05:27Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2267.3518657684326, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:05:29Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, focusing on practicality and getting *something* useful deployed:\n\n*   **Prioritize Simplicity & Speed to Baseline:** Choose the simplest model architecture that can plausibly address the core problem.  Forget cutting-edge; focus on something you can train and deploy *quickly*. A logistic regression or a small decision tree is often better than a complex neural network if it gets you 80% of the way there in 1/10th the time. The goal here is to establish a baseline performance metric ASAP so you have something to compare against.\n\n*   **Leverage Transfer Learning & Pre-trained Models:**  Whenever possible, start with pre-trained models (even if not perfectly suited) and fine-tune them on your specific data. This drastically reduces training time and data requirements. Look for models already trained on similar tasks or datasets. Even a slightly relevant pre-trained model can give you a huge head start compared to building from scratch.\n\n\n\nThese rules emphasize getting *something* working, understanding the limitations quickly, and then iterating if/when resources become available.  They're about pragmatism over perfection in", "prompt_tokens": 25, "completion_tokens": 256, "latency_ms": 47767.552614212036, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, focusing on practicality and getting *something* useful deployed:\n\n*   **Prioritize Simplicity & Speed to Baseline:** Choose the simplest model architecture that can plausibly address the core problem.  Forget cutting-edge; focus on something you can train and deploy *quickly*. A logistic regression or a small decision tree is often better than a complex neural network if it gets you 80% of the way there in 1/10th the time. The goal here is to establish a baseline performance metric ASAP so you have something to compare against.\n\n*   **Leverage Transfer Learning & Pre-trained Models:**  Whenever possible, start with pre-trained models (even if not perfectly suited) and fine-tune them on your specific data. This drastically reduces training time and data requirements. Look for models already trained on similar tasks or datasets. Even a slightly relevant pre-trained model can give you a huge head start compared to building from scratch.\n\n\n\nThese rules emphasize getting *something* working, understanding the limitations quickly, and then iterating if/when resources become available.  They're about pragmatism over perfection in", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:06:17Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 7440.414905548096, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:06:25Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 6961.090326309204, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:06:32Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2535.891056060791, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:06:35Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 47947.67093658447, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:07:23Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5133.731842041016, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:07:28Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this path is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This argument tells the script to connect to the LM Studio server at the specified address and port.\n* `--all-models`:  This crucial argument instructs the script to evaluate *all* models available on the connected server.\n\n**Important Considerations:**\n\n* **Script Existence & Permissions:** Ensure that `scripts/eval_lmstudio_models.py` actually exists in your directory structure, and that it has execute permissions if necessary (e.g., `chmod +x scripts/eval_lmstudio_models.py`).\n* **LM Studio Server Running:**  The LM Studio server *must* be running at `http://12", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 44088.146686553955, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  This invokes the Python interpreter to run the script.\n* `scripts/eval_lmstudio_models.py`: This specifies the path to your evaluation script.  Make sure this path is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`: This argument tells the script to connect to the LM Studio server at the specified address and port.\n* `--all-models`:  This crucial argument instructs the script to evaluate *all* models available on the connected server.\n\n**Important Considerations:**\n\n* **Script Existence & Permissions:** Ensure that `scripts/eval_lmstudio_models.py` actually exists in your directory structure, and that it has execute permissions if necessary (e.g., `chmod +x scripts/eval_lmstudio_models.py`).\n* **LM Studio Server Running:**  The LM Studio server *must* be running at `http://12", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:08:13Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2229.280710220337, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:08:15Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep neural networks.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained models) or models with strong regularization can be lifesavers. Avoid models that demand massive amounts of labeled data if you don't have it, as collecting more data is often the biggest resource sink.\n\n\n\nThese rules emphasize getting something *working* quickly and efficiently rather than aiming for a perfect but unattainable solution given constraints.", "prompt_tokens": 25, "completion_tokens": 206, "latency_ms": 36712.13936805725, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a small Decision Tree before jumping to deep neural networks.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. Techniques like transfer learning (using pre-trained models) or models with strong regularization can be lifesavers. Avoid models that demand massive amounts of labeled data if you don't have it, as collecting more data is often the biggest resource sink.\n\n\n\nThese rules emphasize getting something *working* quickly and efficiently rather than aiming for a perfect but unattainable solution given constraints.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:08:52Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 7895.537376403809, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:09:00Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 7131.544589996338, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:09:08Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2552.302837371826, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:09:11Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads the model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurred.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n    return None\n\n\nif __name__ == '__main__':\n  config = load_model_routing_config()\n\n  if config:\n    print(\"Model routing configuration loaded successfully:\")\n    print(config)\n  else:\n    print(\"Failed to load model", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 48191.633224487305, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads the model routing configuration from a JSON file.\n\n  Args:\n    filepath: The path to the JSON file. Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    A dictionary representing the loaded JSON data, or None if an error occurred.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      data = json.load(f)\n    return data\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n  except Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n    return None\n\n\nif __name__ == '__main__':\n  config = load_model_routing_config()\n\n  if config:\n    print(\"Model routing configuration loaded successfully:\")\n    print(config)\n  else:\n    print(\"Failed to load model", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:09:59Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5309.467315673828, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:10:05Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.\n* `--server-address http://127.0.0.1:1234`: Passes the server address as an argument to the script, telling it where to connect to LM Studio.  This is crucial for accessing the models.\n* `--all-models`: This flag (assuming your script supports it) tells the script to evaluate *all* available models on the specified server.\n\n**Important Considerations and Potential Modifications:**\n\n1. **Script Support:** The `--all-models` flag is an assumption.  You'll need to check the documentation or source code of `scripts/eval_lmstudio_models.py` to confirm if it actually supports this option. If not, you might have to modify the script (or write a wrapper script) to iterate through the models on the server and call the evaluation function for each one individually", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 45455.34253120422, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script.\n* `--server-address http://127.0.0.1:1234`: Passes the server address as an argument to the script, telling it where to connect to LM Studio.  This is crucial for accessing the models.\n* `--all-models`: This flag (assuming your script supports it) tells the script to evaluate *all* available models on the specified server.\n\n**Important Considerations and Potential Modifications:**\n\n1. **Script Support:** The `--all-models` flag is an assumption.  You'll need to check the documentation or source code of `scripts/eval_lmstudio_models.py` to confirm if it actually supports this option. If not, you might have to modify the script (or write a wrapper script) to iterate through the models on the server and call the evaluation function for each one individually", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:10:50Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2475.717782974243, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:10:53Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy. Faster iteration cycles mean you can get feedback quicker and pivot if needed \u2013 crucial when resources are limited.  Don't reach for a cutting-edge architecture unless you have strong evidence it will significantly outperform simpler options *given your data*.\n\n*   **Focus on Data Efficiency:** Select models known to perform well with smaller datasets or those that benefit from transfer learning. Pre-trained models (fine-tuning instead of training from scratch) are excellent choices.  Consider techniques like data augmentation if feasible, but prioritize a model architecture less prone to overfitting in low-data scenarios (e.g., simpler linear models over very deep neural networks).\n\n\n\nThese rules emphasize getting *something useful* deployed quickly and learning from it, rather than aiming for perfection upfront when you can't afford the cost of pursuing it.", "prompt_tokens": 25, "completion_tokens": 238, "latency_ms": 43589.993476867676, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy. Faster iteration cycles mean you can get feedback quicker and pivot if needed \u2013 crucial when resources are limited.  Don't reach for a cutting-edge architecture unless you have strong evidence it will significantly outperform simpler options *given your data*.\n\n*   **Focus on Data Efficiency:** Select models known to perform well with smaller datasets or those that benefit from transfer learning. Pre-trained models (fine-tuning instead of training from scratch) are excellent choices.  Consider techniques like data augmentation if feasible, but prioritize a model architecture less prone to overfitting in low-data scenarios (e.g., simpler linear models over very deep neural networks).\n\n\n\nThese rules emphasize getting *something useful* deployed quickly and learning from it, rather than aiming for perfection upfront when you can't afford the cost of pursuing it.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:11:37Z"}}
{"id": "20260121-003905-guild_route_json", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "prompt_tokens": 33, "completion_tokens": 27, "latency_ms": 6662.646770477295, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"category\":\"code\",\"models\":[\"qwen/qwen3-coder-30b\"]}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:11:44Z"}}
{"id": "20260121-003905-guild_task_summary", "task_type": "chat", "system_prompt": null, "user_prompt": "Summarize the purpose of a task routing system in one sentence.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "prompt_tokens": 22, "completion_tokens": 34, "latency_ms": 8255.596160888672, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "A task routing system intelligently distributes incoming work items to the most appropriate person or team based on predefined rules and criteria, ensuring efficient completion and optimized resource allocation. \n\n\n\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:11:52Z"}}
{"id": "20260121-003905-guild_reasoning_priority", "task_type": "reasoning", "system_prompt": null, "user_prompt": "If a task is marked urgent and requires code generation, which routing category should it use? Reply with one word.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Code.\n", "prompt_tokens": 32, "completion_tokens": 4, "latency_ms": 2650.9058475494385, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Code.\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:11:55Z"}}
{"id": "20260121-003905-guild_code_config", "task_type": "code", "system_prompt": null, "user_prompt": "Write a Python snippet that loads JSON from 'guild/config/model_routing.json'.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:", "prompt_tokens": 28, "completion_tokens": 256, "latency_ms": 51713.10544013977, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```python\nimport json\n\ndef load_model_routing_config(filepath='guild/config/model_routing.json'):\n  \"\"\"Loads model routing configuration from a JSON file.\n\n  Args:\n    filepath (str, optional): The path to the JSON file. \n                               Defaults to 'guild/config/model_routing.json'.\n\n  Returns:\n    dict: A dictionary containing the loaded configuration data.\n          Returns None if the file is not found or invalid JSON.\n  \"\"\"\n  try:\n    with open(filepath, 'r') as f:\n      config = json.load(f)\n    return config\n  except FileNotFoundError:\n    print(f\"Error: File not found at {filepath}\")\n    return None\n  except json.JSONDecodeError:\n    print(f\"Error: Invalid JSON format in {filepath}\")\n    return None\n\n# Example usage:\nconfig_data = load_model_routing_config()\n\nif config_data:\n  print(\"Model routing configuration loaded successfully:\")\n  print(config_data)\nelse:\n  print(\"Failed to load model routing configuration.\")\n```\n\nKey improvements and explanations:", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:12:47Z"}}
{"id": "20260121-003905-guild_format_task", "task_type": "format", "system_prompt": null, "user_prompt": "Return ONLY JSON: {\"task_id\":\"T-100\",\"status\":\"queued\"}", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "prompt_tokens": 28, "completion_tokens": 22, "latency_ms": 5622.502088546753, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```json\n{\"task_id\":\"T-100\",\"status\":\"queued\"}\n```\n", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:12:53Z"}}
{"id": "20260121-003905-guild_code_cli", "task_type": "code", "system_prompt": null, "user_prompt": "Write a bash command to run scripts/eval_lmstudio_models.py against all models on http://127.0.0.1:1234.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script. Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`:  Passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`: This is the key part! It instructs the script to evaluate *all* models available on the specified server. The script needs to be designed to recognize and handle this flag; most evaluation scripts have a way to specify either specific model IDs or all models.\n\n**Important Considerations:**\n\n* **Script Functionality:**  This command assumes that `scripts/eval_lmstudio_models.py` is written to accept `--all-models` as an argument and correctly iterates through the available models on the LM Studio server at `http://1", "prompt_tokens": 46, "completion_tokens": 256, "latency_ms": 48354.443073272705, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "```bash\npython scripts/eval_lmstudio_models.py --server-address http://127.0.0.1:1234 --all-models\n```\n\n**Explanation:**\n\n* `python`:  Executes the Python interpreter.\n* `scripts/eval_lmstudio_models.py`: Specifies the path to your evaluation script. Make sure this is correct relative to where you're running the command from.\n* `--server-address http://127.0.0.1:1234`:  Passes the server address as an argument to the script, telling it where to connect to LM Studio.\n* `--all-models`: This is the key part! It instructs the script to evaluate *all* models available on the specified server. The script needs to be designed to recognize and handle this flag; most evaluation scripts have a way to specify either specific model IDs or all models.\n\n**Important Considerations:**\n\n* **Script Functionality:**  This command assumes that `scripts/eval_lmstudio_models.py` is written to accept `--all-models` as an argument and correctly iterates through the available models on the LM Studio server at `http://1", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:13:42Z"}}
{"id": "20260121-003905-guild_reasoning_fallback", "task_type": "reasoning", "system_prompt": null, "user_prompt": "Name one valid fallback model from this list: gemma-3n-e4b, lfm2.5-1.2b. Reply with the model id only.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "prompt_tokens": 46, "completion_tokens": 11, "latency_ms": 2433.077096939087, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "gemma-3n-e4b\n", "rationale": "single-teacher"}, "labels": {"difficulty": "medium", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:13:44Z"}}
{"id": "20260121-003905-guild_chat_policy", "task_type": "chat", "system_prompt": null, "user_prompt": "Give two bullet rules for how Guild should pick a model when resources are tight.", "context": null, "attachments": [], "tools": [], "teacher_responses": [{"model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a basic Random Forest before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. This could include techniques like transfer learning (leveraging pre-trained weights), regularized linear models, or even simpler algorithms that don't overfit easily. Avoid models requiring massive amounts of labeled data if you can\u2019t realistically obtain it.\n\n\n\nThese rules emphasize getting something *working* and learning from it quickly, rather than aiming for a perfect solution that may never materialize due to resource constraints.", "prompt_tokens": 25, "completion_tokens": 211, "latency_ms": 40103.710889816284, "checks": {"passed": 0, "total": 0}, "judge_scores": []}], "selection": {"winner_model": "google/gemma-3-27b", "response": "Okay, here are two bullet-point rules for Guild picking a model when resources (time, compute, data) are tight, geared towards maximizing impact:\n\n*   **Prioritize Simplicity & Speed of Iteration:** Choose the *simplest* model that has a reasonable chance of meeting your core requirements. Complex models take longer to train, debug, and deploy.  Faster iteration cycles mean you can get feedback quicker and pivot if needed \u2013 crucial when resources are limited. Think Logistic Regression or a basic Random Forest before jumping to deep learning.\n\n*   **Focus on Data Efficiency:** Select models known for performing well with *smaller datasets*. This could include techniques like transfer learning (leveraging pre-trained weights), regularized linear models, or even simpler algorithms that don't overfit easily. Avoid models requiring massive amounts of labeled data if you can\u2019t realistically obtain it.\n\n\n\nThese rules emphasize getting something *working* and learning from it quickly, rather than aiming for a perfect solution that may never materialize due to resource constraints.", "rationale": "single-teacher"}, "labels": {"difficulty": "easy", "style": "neutral", "safety": "safe"}, "metadata": {"domain": "guild", "source": "seed_script", "prompt_set": "scripts/eval/prompts_guild.json", "run_id": "20260121-003905", "created_at": "2026-01-21T01:14:25Z"}}
